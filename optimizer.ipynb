{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "\n",
        "from os.path import exists\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "RKWYK1eLEnXB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer classes"
      ],
      "metadata": {
        "id": "bvOpByxEEhnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dfov0WGQARXo"
      },
      "outputs": [],
      "source": [
        "class signSGD(optim.Optimizer):\n",
        "  \"\"\"Original implementation of signSGD from https://github.com/jxbz/signSGD/blob/master/signSGD_zeros.ipynb\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params, lr=0.01, rand_zero=True):\n",
        "    defaults = dict(lr=lr)\n",
        "    self.rand_zero = rand_zero\n",
        "    super(signSGD, self).__init__(params, defaults)\n",
        "\n",
        "  def step(self, closure=None):\n",
        "    \"\"\"Performs a single optimization step.\n",
        "    Arguments:\n",
        "        closure (callable, optional): A closure that reevaluates the model\n",
        "            and returns the loss.\n",
        "    \"\"\"\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "\n",
        "        # take sign of gradient\n",
        "        grad = torch.sign(p.grad)\n",
        "\n",
        "        # randomise zero gradients to Â±1\n",
        "        if self.rand_zero:\n",
        "          grad[grad==0] = torch.randint_like(grad[grad==0], low=0, high=2)*2 - 1\n",
        "          assert not (grad==0).any()\n",
        "        \n",
        "        # make update\n",
        "        p.data -= group['lr'] * grad\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "class compressedSGD(optim.Optimizer):\n",
        "  \"\"\"Generalized gradient compression using binning.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params, lr=0.01, rand_zero=True, num_bits=2, decay_max=1.0, decay_min=1.0):\n",
        "\n",
        "    if num_bits <= 0 or type(num_bits) != int:\n",
        "       raise ValueError(\"Expected num_bits to be a positive integer.\")\n",
        "    if decay_max > 1 or decay_max <= 0 or decay_min > 1 or decay_min <= 0:\n",
        "      raise ValueError(\"Expected decay_max and decay_min to be a float in the interval (0,1].\")\n",
        "\n",
        "    defaults = dict(lr=lr)\n",
        "    super(compressedSGD, self).__init__(params, defaults)\n",
        "    self.decay_max = decay_max\n",
        "    self.decay_min = decay_min\n",
        "    self.max_grad_vals = {}\n",
        "    self.min_grad_vals = {}\n",
        "    self.num_bits=num_bits\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        self.max_grad_vals[p] = None\n",
        "        self.min_grad_vals[p] = None\n",
        "\n",
        "  def step(self, closure=None):\n",
        "    \"\"\"Performs a single optimization step.\n",
        "    Arguments:\n",
        "        closure (callable, optional): A closure that reevaluates the model\n",
        "            and returns the loss.\n",
        "    \"\"\"\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      loss = closure()\n",
        "\n",
        "    # parameter update\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "\n",
        "        # set the max and min vals for gradients\n",
        "        if self.max_grad_vals[p] == None:\n",
        "          self.max_grad_vals[p] = torch.clone(p.grad)\n",
        "        else:\n",
        "          self.max_grad_vals[p] = torch.max(p.grad, \n",
        "                                            self.max_grad_vals[p] * self.decay_max)\n",
        "\n",
        "        if self.min_grad_vals[p] == None:\n",
        "          self.min_grad_vals[p] = torch.clone(p.grad)\n",
        "        else:\n",
        "          self.min_grad_vals[p] = torch.min(p.grad, \n",
        "                                            self.min_grad_vals[p] * self.decay_min)\n",
        "        # descretize gradient based on max_grad_val, if gradient > 0\n",
        "        # and based on min_grad_val, if gradient < 0\n",
        "\n",
        "        grad = p.grad\n",
        "        grad[grad > 0] = torch.ceil((2**(self.num_bits-1)) * torch.div(grad[grad > 0], self.max_grad_vals[p][grad > 0]))\n",
        "        grad[grad < 0] = - torch.ceil((2**(self.num_bits-1)) * torch.div(grad[grad < 0], self.min_grad_vals[p][grad < 0]))\n",
        "        # make update\n",
        "        p.data -= group['lr'] * grad\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tests for compressedSGN"
      ],
      "metadata": {
        "id": "czCFdKIH4KRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def test_max():\n",
        "  A = Variable(torch.tensor([[1.0,2.0],[3.0,4.0]]), requires_grad=True)\n",
        "  optimizer = compressedSGD([A], lr=1, num_bits=2, decay_max=1.0, decay_min=1.0)\n",
        "\n",
        "  x = Variable(torch.tensor([[1.0],[1.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[1.0],[1.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  assert torch.equal(optimizer.max_grad_vals[A], torch.tensor([[1.0,1.0],[1.0,1.0]]) ), \"Expected max grad tensor for A to be [[1.0,1.0],[1.0,1.0]]\"\n",
        "  assert torch.equal(A, torch.tensor([[-1.0,0.0],[1.0,2.0]]) ), \"Expected A to be [[-1.0,0.0],[1.0,2.0]]\"\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[2.0],[2.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[2.0],[2.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  assert torch.equal(optimizer.max_grad_vals[A], torch.tensor([[2.0,2.0],[2.0,2.0]]) ), \"Expected max grad tensor for A to be [[2.0,2.0],[2.0,2.0]]\"\n",
        "  assert torch.equal(A, torch.tensor([[-3.0,-2.0],[-1.0,0.0]]) ), \"Expected max grad tensor for A to be [[-1.0,0.0],[1.0,2.0]]\"\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[1.0],[3.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[1.0],[3.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  assert torch.equal(optimizer.max_grad_vals[A], torch.tensor([[2.0,3.0],[2.0,3.0]]) ), \"Expected max grad tensor for A to be [[2.0,3.0],[2.0,3.0]]\"\n",
        "  assert torch.equal(A, torch.tensor([[-4.0,-4.0],[-2.0,-2.0]]) ), \"Expected max grad tensor for A to be [[-1.0,0.0],[1.0,2.0]]\"\n",
        "\n",
        "def test_min():\n",
        "  A = Variable(torch.tensor([[1.0,2.0],[3.0,4.0]]), requires_grad=True)\n",
        "  optimizer = compressedSGD([A], lr=1, num_bits=2, decay_max=1.0, decay_min=1.0)\n",
        "\n",
        "  x = Variable(torch.tensor([[-1.0],[-1.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[-1.0],[-1.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  assert torch.equal(optimizer.min_grad_vals[A], torch.tensor([[-1.0,-1.0],[-1.0,-1.0]]) ), \"Expected min grad tensor for A to be [[-1.0,-1.0],[-1.0,-1.0]]\"\n",
        "  assert torch.equal(A, torch.tensor([[3.0,4.0],[5.0,6.0]]) ), \"Expected A to be [[3.0,4.0],[5.0,6.0]]\"\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[-2.0],[-2.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[2.0],[2.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  assert torch.equal(optimizer.min_grad_vals[A], torch.tensor([[-2.0,-2.0],[-2.0,-2.0]]) )\n",
        "  assert torch.equal(A, torch.tensor([[5.0,6.0],[7.0,8.0]]) )\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[-1.0],[-3.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[1.0],[3.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  assert torch.equal(optimizer.min_grad_vals[A], torch.tensor([[-2.0,-3.0],[-2.0,-3.0]]) )\n",
        "  assert torch.equal(A, torch.tensor([[6.0,8.0],[8.0,10.0]]) )\n",
        "  \n",
        "def test_lr():\n",
        "  A = Variable(torch.tensor([[1.0,2.0],[3.0,4.0]]), requires_grad=True)\n",
        "  optimizer = compressedSGD([A], lr=0.1, num_bits=2, decay_max=1.0, decay_min=1.0)\n",
        "\n",
        "  x = Variable(torch.tensor([[-1.0],[-1.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[-1.0],[-1.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  assert torch.equal(A, torch.tensor([[1.2,2.2],[3.2,4.2]]) )\n",
        "\n",
        "def test_bins():\n",
        "  A = Variable(torch.tensor([[1.0,2.0],[3.0,4.0]]), requires_grad=True)\n",
        "  optimizer = compressedSGD([A], lr=1, num_bits=3, decay_max=1.0, decay_min=1.0)\n",
        "\n",
        "  x = Variable(torch.tensor([[-1.0],[-1.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[-1.0],[-1.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  assert torch.equal(A, torch.tensor([[5.0,6.0],[7.0,8.0]]) )\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[-1.0],[-3.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[1.0],[3.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  assert torch.equal(A, torch.tensor([[9.0,10.0],[11.0,12.0]]) )\n",
        "\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[-2.0],[-1.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[1.0],[3.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  assert torch.equal(A, torch.tensor([[13.0,12.0],[15.0,14.0]]) )\n",
        "\n",
        "def test_decay():\n",
        "  A = Variable(torch.tensor([[1.0,2.0],[3.0,4.0]]), requires_grad=True)\n",
        "  optimizer = compressedSGD([A], lr=1, num_bits=2, decay_max=0.5, decay_min=0.5)\n",
        "\n",
        "  x = Variable(torch.tensor([[5.0],[5.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[5.0],[5.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  assert torch.equal(optimizer.max_grad_vals[A], torch.tensor([[5.0,5.0],[5.0,5.0]]) )\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[1.0],[1.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[1.0],[3.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  assert torch.equal(optimizer.max_grad_vals[A], torch.tensor([[2.5,2.5],[2.5,2.5]]) )\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  x = Variable(torch.tensor([[5.0],[1.0]]), requires_grad=False)\n",
        "  y = A @ x\n",
        "  loss = torch.sum(y-Variable(torch.tensor([[1.0],[3.0]]), requires_grad=False))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  assert torch.equal(optimizer.max_grad_vals[A], torch.tensor([[5.0,1.25],[5.0,1.25]]) )\n",
        "  \n",
        "def test_compressedSGD():\n",
        "  test_max()\n",
        "  test_min()\n",
        "  test_lr()\n",
        "  test_bins()\n",
        "  test_decay()\n",
        "  \n",
        "test_compressedSGD()"
      ],
      "metadata": {
        "id": "Y8oxikmI4PtG"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models to be trained"
      ],
      "metadata": {
        "id": "6hw0ww7rIYLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_MNIST(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(CNN_MNIST, self).__init__()\n",
        "    # Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    # padding, so the produced convolutions have orginal size\n",
        "    self.conv = nn.Conv2d(1, 32, 3, 1, 1, padding_mode = 'zeros')\n",
        "\n",
        "    # MaxPool2D(kernel_size, stride)\n",
        "    self.maxpool = nn.MaxPool2d(2, stride=2) # stride's default value is kernel size\n",
        "\n",
        "    self.dropout1 = nn.Dropout(p=0.5)\n",
        "    self.fc1 = nn.Linear(14*14*32, 512)\n",
        "    self.dropout2 = nn.Dropout(p=0.5)\n",
        "    self.fc2 = nn.Linear(512, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "      x = self.conv(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.maxpool(x)\n",
        "      x = torch.flatten(x,1)\n",
        "      x = self.dropout1(x)\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.dropout2(x)\n",
        "      x = self.fc2(x)\n",
        "      return(x)\n"
      ],
      "metadata": {
        "id": "BTuSI9j8Iao-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for training the model."
      ],
      "metadata": {
        "id": "EbRhc4UNEcAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, data_loader, optimizer, epoch, track_losses = False,\n",
        "              log_interval=100):\n",
        "  \n",
        "  model.train()\n",
        "  losses = []\n",
        "  for batch_idx, (data, target) in enumerate(data_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad() # zero gradients for every batch \n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target) # cross entropy loss \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(data_loader.dataset),\n",
        "        100. * batch_idx / len(data_loader), loss.item()))\n",
        "    \n",
        "    if track_losses:\n",
        "      losses.append(loss.data.item())\n",
        "  \n",
        "  return losses\n",
        "\n",
        "\n",
        "def test(model, device, data_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in data_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      # cross entropy loss, sum up batch loss\n",
        "      test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)  # get the index of maximum prediction\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(data_loader.dataset)\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      test_loss, correct, len(data_loader.dataset),\n",
        "      100. * correct / len(data_loader.dataset)))\n",
        "  return (test_loss,  100. * correct / len(data_loader.dataset))"
      ],
      "metadata": {
        "id": "bt_7-K40D3T5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function:"
      ],
      "metadata": {
        "id": "kR6ZZ2WAEuFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_MNIST(\n",
        "         num_classes,\n",
        "         model_id,\n",
        "         optimizer_choice = \"compressedSGD\",\n",
        "         MODELPATH = './',\n",
        "         save_interval = 25,\n",
        "         epochs = 10,\n",
        "         train_batch_size = 64,\n",
        "         test_batch_size = 64,\n",
        "         learning_rate = 0.01,\n",
        "         use_scheduler = True,\n",
        "         gamma = 0.7,\n",
        "         step_size = 10,\n",
        "         num_bits = 2,\n",
        "         beta = 1,\n",
        "         seed = 0,\n",
        "         track_training_loss = False):\n",
        "  \"\"\"\n",
        "  Loads a model from MODELPATH/<label>_model_<model_id>.pt, if the directory exists, otherwise trains a fresh model.\n",
        "  Args:\n",
        "    num_classes (int): Output dimension of the CNN, i.e. number of different classes to predict.\n",
        "    model_id (string): ModelID, is used to determine the outputfile to save the model.\n",
        "    optimizer_choice (string): Choice of optimizer to be used. Has to be 'SGD', 'signSGD' or 'compressedSGD'.\n",
        "    MODELPATH (string): Directory to indicate where the model should be saved.\n",
        "    save_interval (int): Number of epochs until the model is saved.\n",
        "    epochs (int): Number of epochs to train, after these epochs the function terminates.\n",
        "    train_batch_size (int): Batchsize when training the model.\n",
        "    test_batch_size (int): Batchsize when evaluating the model.\n",
        "    learning_rate (float): Learning rate used in the optimizer.\n",
        "    use_scheduler (bool): Boolean indicating if an exponential scheduler should be used to decay weight of training paramerters. E.g. for decaying learning rate.\n",
        "    gamma (float): Decay factor if scheduler is used.\n",
        "    step_size (int): Number of steps after which the weight decay of the scheduler is applied.  \n",
        "    num_bits (int): Number of bits for the communication in compressedSGD. Based on this number, the number of bins is 2^num_bits (+ 1).\n",
        "      (+1) if 0 is included when binning.\n",
        "    beta (float): Decay factor for max gradient and min gradient in compressedSGD.\n",
        "    seed (int): Random seed to be set before training.\n",
        "    track_training_loss (bool): Boolean indicating if training loss should be tracked and returned.\n",
        "  Out:\n",
        "    Returns the training losses, if tracked.\n",
        "    Saves the model to at 'MODELPATH/<label>_model_<model_id>.pt'\n",
        "  \"\"\"\n",
        "  if optimizer_choice not in {\"SGD\", \"signSGD\", \"compressedSGD\"}:\n",
        "       raise ValueError(\"Must choose an optimizer out of 'SGD', 'signSGD' or 'compressedSGD'.\")\n",
        "\n",
        "  PATH = MODELPATH + \"_model_\" + model_id + \".pt\"\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  train_kwargs = {'batch_size': train_batch_size,\n",
        "                  'shuffle': True}\n",
        "  test_kwargs = {'batch_size': test_batch_size}\n",
        "  # preprocess the data\n",
        "\n",
        "  # load the training and test data\n",
        "  training_data = torchvision.datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,                         \n",
        "    transform = torchvision.transforms.ToTensor(), \n",
        "    download = True)\n",
        "\n",
        "  test_data = torchvision.datasets.MNIST(\n",
        "    root = 'data', \n",
        "    train = False, \n",
        "    transform =  torchvision.transforms.ToTensor())\n",
        "  \n",
        "  # identify GPU usage\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  print(\"Cuda Available: \" + str(use_cuda))\n",
        "  if use_cuda:\n",
        "    print(\"GPU Count: \" + str(torch.cuda.device_count()))\n",
        "\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(training_data,**train_kwargs)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, **test_kwargs)\n",
        "  \n",
        "  model = CNN_MNIST(num_classes)\n",
        "  \n",
        "  \n",
        "\n",
        "  # load parameters, if file exists otherwise initialize weights\n",
        "  epoch_num = 0\n",
        "  validation_errors = []\n",
        "  if exists(PATH):\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    epoch_num = checkpoint['epoch']\n",
        "    validation_errors = checkpoint['loss']\n",
        "  else:\n",
        "    model.to(device)\n",
        "    \n",
        "  # initialize the optimizer\n",
        "  optimizer = None\n",
        "  if optimizer_choice == \"SGD\":\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  elif optimizer_choice == \"signSGD\":\n",
        "    optimizer = signSGD(model.parameters(), lr=learning_rate)\n",
        "  elif optimizer_choice == \"compressedSGD\":\n",
        "    optimizer = compressedSGD(model.parameters(), lr=learning_rate, num_bits=num_bits, decay_max=beta, decay_min=beta) \n",
        "\n",
        "  \n",
        "  # Decays the learning rate of each parameter group by gamma every step_size epochs.\n",
        "  scheduler = None\n",
        "  if use_scheduler:\n",
        "     scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma) \n",
        "\n",
        "  # for statistics\n",
        "  t0 = time.time()\n",
        "  tlast = t0\n",
        "\n",
        "  for epoch in range(epoch_num + 1, epoch_num + epochs + 1):\n",
        "    loss = []\n",
        "    if track_training_loss:\n",
        "      loss += train(model, device, train_loader, optimizer, epoch, track_losses = True)\n",
        "    else:\n",
        "      train(model, device, train_loader, optimizer, epoch, track_losses = False)\n",
        "    verror = test(model, device, test_loader)\n",
        "    validation_errors.append(verror)\n",
        "\n",
        "    td = time.time() - tlast\n",
        "    tds = time.time() - t0\n",
        "    tds_m = int(tds // 60)\n",
        "    tds_s = int(tds) % 60\n",
        "    tlast = time.time()\n",
        "\n",
        "    with open(\"report.txt\", \"a\") as myfile:\n",
        "      # epoch, loss, accuracy\n",
        "      myfile.write('{},{:.4f},{:.4f}%,{:8.2f}s,{:8d}m{:d}s\\n'.format(epoch, verror[0], verror[1], td, tds_m, tds_s))\n",
        "    print(\"{:8.2f}s since last epoch\\n{:8d}m{:d}s since start\".format((td), tds_m, tds_s))\n",
        "\n",
        "    if epoch % save_interval == 0:\n",
        "      torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'loss': validation_errors,\n",
        "            }, PATH)\n",
        "    if use_scheduler:\n",
        "      # Decays the learning rate of each parameter group by gamma every step_size epochs.\n",
        "      scheduler.step()\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gJXOP7TkEtFS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to visualize training loss of compressedSGD:"
      ],
      "metadata": {
        "id": "yb_qxDNP0nqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_loss_cSGD(learning_rates, betas, num_bits, save_fig = False, file='compressedSGD_MNIST.pdf'):\n",
        "  loss = {}\n",
        "  for lr in learning_rates:\n",
        "    for b in betas:\n",
        "      loss[(lr, b)] = train_model_MNIST(\n",
        "      num_classes = 10,\n",
        "      model_id = \"1\",\n",
        "      optimizer_choice = \"compressedSGD\",\n",
        "      epochs = 1,\n",
        "      train_batch_size = 64,\n",
        "      test_batch_size = 64,\n",
        "      learning_rate = lr,\n",
        "      use_scheduler = False,\n",
        "      gamma = 0.7,\n",
        "      step_size = 1,\n",
        "      num_bits = num_bits,\n",
        "      beta = b,\n",
        "      seed = 1337,\n",
        "      track_training_loss = True)\n",
        "\n",
        "  x = np.linspace(0, len(loss[(learning_rates[0], weight_decay[0])]), num = len(loss[(learning_rates[0], weight_decay[0])]))\n",
        "\n",
        "  fig, axs = plt.subplots(len(learning_rates), len(weight_decay), figsize=(20, 15))\n",
        "\n",
        "  for i in range(len(learning_rates)):\n",
        "    for j in range(len(weight_decay)):\n",
        "      axs[i, j].plot(x, loss[(learning_rates[i], weight_decay[j])])\n",
        "      axs[i, j].set_ylim([0, 4])\n",
        "      axs[i, j].title.set_text('learning rate: ' + str(learning_rates[i]) + ', beta: ' +  str(weight_decay[j]))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(file)\n",
        "\n",
        "learning_rates = [0.01, 0.001, 0.0005, 0.0001, 0.00005]\n",
        "weight_decay = [1, 0.8, 0.5, 0.3, 0.1]   \n",
        "plot_training_loss_cSGD(learning_rates, weight_decay, 3, save_fig=True)"
      ],
      "metadata": {
        "id": "FdscIS_lJKQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to visualize training loss of SGD/signSGD"
      ],
      "metadata": {
        "id": "TYB7Cprv02Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_loss_SGD(learning_rates, optimizer_choice = \"SGD\", save_fig = False, file='_MNIST.pdf'):\n",
        "  loss = {}\n",
        "  for lr in learning_rates:\n",
        "    loss[lr] = train_model_MNIST(\n",
        "    num_classes = 10,\n",
        "    model_id = \"1\",\n",
        "    optimizer_choice = optimizer_choice,\n",
        "    epochs = 1,\n",
        "    train_batch_size = 64,\n",
        "    test_batch_size = 64,\n",
        "    learning_rate = lr,\n",
        "    use_scheduler = False,\n",
        "    gamma = 0.7,\n",
        "    step_size = 1,\n",
        "    seed = 1337,\n",
        "    track_training_loss = True)\n",
        "\n",
        "   \n",
        "  x = np.linspace(0, len(loss[(learning_rates[0])]), num = len(loss[(learning_rates[0])]))\n",
        "\n",
        "  fig, axs = plt.subplots(len(learning_rates), figsize=(5, 15))\n",
        "\n",
        "  for i in range(len(learning_rates)):\n",
        "    axs[i].plot(x, loss[learning_rates[i]])\n",
        "    axs[i].set_ylim([0, 4])\n",
        "    axs[i].title.set_text('learning rate: ' + str(learning_rates[i]))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(optimizer_choice + file)\n",
        "\n",
        "learning_rates = [0.5, 0.1, 0.01, 0.001, 0.0005, 0.0001, 0.00005]\n",
        "plot_training_loss_SGD(learning_rates, optimizer_choice=\"SGD\", save_fig=True)"
      ],
      "metadata": {
        "id": "PssmzFDx0mc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_MNIST(\n",
        "     num_classes = 10,\n",
        "     model_id = \"1\",\n",
        "     optimizer_choice = \"SGD\",\n",
        "     save_interval = 25, \n",
        "     epochs = 1,\n",
        "     train_batch_size = 64,\n",
        "     test_batch_size = 64,\n",
        "     learning_rate = 0.1,\n",
        "     use_scheduler = False,\n",
        "     gamma = 0.7,\n",
        "     step_size = 1,\n",
        "     num_bits = 2,\n",
        "     beta = 0.9,\n",
        "     seed = 1337,\n",
        "     track_training_loss = True)"
      ],
      "metadata": {
        "id": "EkxqWIioJJIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}